{"version":3,"file":"index.js","sources":["../src/shared/client.ts","../src/shared/factory.ts","../src/shared/message.ts","../src/shared/thread.ts","../src/shared/types.ts"],"sourcesContent":["import { ApiError, ValidationError } from '@have/utils';\nimport OpenAI from 'openai';\n\nimport type { AIMessageOptions } from './message';\n\n/**\n * Common options for AI client configuration\n */\nexport interface AIClientOptions {\n  /**\n   * Type of AI client (e.g., 'openai')\n   */\n  type?: string;\n\n  /**\n   * Response format for AI completions\n   */\n  responseFormat?: string;\n\n  /**\n   * API key for authentication\n   */\n  apiKey?: string;\n\n  /**\n   * Base URL for API requests\n   */\n  baseUrl?: string;\n}\n\n/**\n * Interface defining required methods for AI clients\n */\nexport interface AIClientInterface {\n  /**\n   * Configuration options for this client\n   */\n  options: AIClientOptions;\n\n  /**\n   * Sends a message to the AI and gets a response\n   *\n   * @param text - Message text\n   * @param options - Message options\n   * @returns Promise resolving to the AI response\n   */\n  message(text: string, options: AIMessageOptions): Promise<unknown>;\n\n  /**\n   * Gets a text completion from the AI\n   *\n   * @param text - Input text for completion\n   * @param options - Completion options\n   * @returns Promise resolving to the completion result\n   */\n  textCompletion(text: string, options: AIMessageOptions): Promise<unknown>;\n}\n\n/**\n * Type guard to check if options are for OpenAI client\n *\n * @param options - Options to check\n * @returns True if options are valid for OpenAI client\n */\nfunction isOpenAIClientOptions(\n  options: AIClientOptions,\n): options is OpenAIClientOptions {\n  return options.type === 'openai' && 'apiKey' in options && !!options.apiKey;\n}\n\n/**\n * Type guard to check if value is an AI client instance\n *\n * @param value - Value to check\n * @returns True if value is an AI client instance\n */\nfunction isAIClientInstance(value: any): value is AIClient {\n  return value instanceof AIClient;\n}\n\n/**\n * Options for AI text completion requests\n */\nexport interface AITextCompletionOptions {\n  /**\n   * Model identifier to use\n   */\n  model?: string;\n\n  /**\n   * Timeout in milliseconds\n   */\n  timeout?: number;\n\n  /**\n   * Role of the message sender\n   */\n  role?: OpenAI.Chat.ChatCompletionRole;\n\n  /**\n   * Previous messages in the conversation\n   */\n  history?: OpenAI.Chat.ChatCompletionMessageParam[];\n\n  /**\n   * Name of the message sender\n   */\n  name?: string;\n\n  /**\n   * Penalty for token frequency\n   */\n  frequencyPenalty?: number;\n\n  /**\n   * Token bias adjustments\n   */\n  logitBias?: Record<string, number>;\n\n  /**\n   * Whether to return log probabilities\n   */\n  logprobs?: boolean;\n\n  /**\n   * Number of top log probabilities to return\n   */\n  topLogprobs?: number;\n\n  /**\n   * Maximum tokens to generate\n   */\n  maxTokens?: number;\n\n  /**\n   * Number of completions to generate\n   */\n  n?: number;\n\n  /**\n   * Penalty for token presence\n   */\n  presencePenalty?: number;\n\n  /**\n   * Format for the response\n   */\n  responseFormat?: { type: 'text' | 'json_object' };\n\n  /**\n   * Random seed for deterministic results\n   */\n  seed?: number;\n\n  /**\n   * Sequences that stop generation\n   */\n  stop?: string | Array<string>;\n\n  /**\n   * Whether to stream responses\n   */\n  stream?: boolean;\n\n  /**\n   * Sampling temperature\n   */\n  temperature?: number;\n\n  /**\n   * Top-p sampling parameter\n   */\n  topProbability?: number;\n\n  /**\n   * Available tools for the model\n   */\n  tools?: Array<any>; // todo: figure out generic solution - Array<OpenAI.Chat.ChatCompletionTool>;\n\n  /**\n   * Tool selection behavior\n   */\n  toolChoice?:\n    | 'none'\n    | 'auto'\n    | { type: 'function'; function: { name: string } };\n\n  /**\n   * User identifier\n   */\n  user?: string;\n\n  /**\n   * Callback for handling streaming responses\n   */\n  onProgress?: (partialMessage: string) => void;\n}\n\n/**\n * Base class for AI clients\n * Provides a common interface for different AI service providers\n */\nexport class AIClient {\n  /**\n   * Configuration options for this client\n   */\n  public options: AIClientOptions;\n\n  /**\n   * Creates a new AIClient\n   *\n   * @param options - Client configuration options\n   */\n  constructor(options: AIClientOptions) {\n    this.options = options;\n  }\n\n  /**\n   * Sends a message to the AI\n   * Base implementation returns a placeholder response\n   *\n   * @param text - Message text\n   * @param options - Message options\n   * @returns Promise resolving to a placeholder response\n   */\n  public async message(\n    _text: string,\n    _options: AITextCompletionOptions = { role: 'user' },\n  ) {\n    return 'not a real ai message, this is the base class!';\n  }\n\n  /**\n   * Factory method to create appropriate AI client based on options\n   *\n   * @param options - Client configuration options\n   * @returns Promise resolving to an initialized AI client\n   * @throws Error if client type is invalid\n   */\n  public static async create(\n    options: AIClientOptions | AIClient,\n  ): Promise<AIClient | OpenAIClient> {\n    // If an AI client instance is passed, return it directly\n    if (isAIClientInstance(options)) {\n      return options;\n    }\n\n    // Cast to options since we know it's not an instance\n    const clientOptions = options as AIClientOptions;\n\n    if (isOpenAIClientOptions(clientOptions)) {\n      return OpenAIClient.create(clientOptions);\n    }\n\n    // Delegate to modern factory for non-OpenAI providers\n    const providedType = (clientOptions as any).type;\n    if (providedType && providedType !== 'openai') {\n      const { getAI } = await import('./factory.js');\n      return (await getAI(clientOptions as any)) as any;\n    }\n\n    // Provide specific error messages for common issues\n    if (providedType === 'openai') {\n      throw new ValidationError(\n        'OpenAI API key is required but missing or empty',\n        {\n          supportedTypes: [\n            'openai',\n            'anthropic',\n            'gemini',\n            'bedrock',\n            'huggingface',\n          ],\n          providedType,\n          hint: 'Set OPENAI_API_KEY environment variable or pass apiKey in options',\n        },\n      );\n    }\n\n    throw new ValidationError('Invalid client type specified', {\n      supportedTypes: [\n        'openai',\n        'anthropic',\n        'gemini',\n        'bedrock',\n        'huggingface',\n      ],\n      providedType,\n    });\n  }\n\n  /**\n   * Gets a text completion from the AI\n   * In base class, delegates to message method\n   *\n   * @param text - Input text for completion\n   * @param options - Completion options\n   * @returns Promise resolving to the completion result\n   */\n  public textCompletion(\n    text: string,\n    options: AITextCompletionOptions = {\n      role: 'user',\n    },\n  ) {\n    return this.message(text, options);\n  }\n}\n\n/**\n * Creates an OpenAI client instance\n *\n * @param options - OpenAI configuration options\n * @returns Promise resolving to an OpenAI client\n */\nexport async function getOpenAI(options: {\n  apiKey?: string;\n  baseUrl?: string;\n}) {\n  return new OpenAI({\n    apiKey: options.apiKey,\n    baseURL: options.baseUrl,\n  });\n}\n\n/**\n * Options specific to OpenAI text completion requests\n */\nexport interface OpenAITextCompletionOptions {\n  /**\n   * Model identifier to use\n   */\n  model?: string;\n\n  /**\n   * Timeout in milliseconds\n   */\n  timeout?: number;\n\n  /**\n   * Role of the message sender\n   */\n  role?: OpenAI.Chat.ChatCompletionRole;\n\n  /**\n   * Previous messages in the conversation\n   */\n  history?: Array<OpenAI.Chat.ChatCompletionMessageParam>;\n\n  /**\n   * Name of the message sender\n   */\n  name?: string;\n\n  /**\n   * Penalty for token frequency\n   */\n  frequencyPenalty?: number;\n\n  /**\n   * Token bias adjustments\n   */\n  logitBias?: Record<string, number>;\n\n  /**\n   * Whether to return log probabilities\n   */\n  logprobs?: boolean;\n\n  /**\n   * Number of top log probabilities to return\n   */\n  topLogprobs?: number;\n\n  /**\n   * Maximum tokens to generate\n   */\n  maxTokens?: number;\n\n  /**\n   * Number of completions to generate\n   */\n  n?: number;\n\n  /**\n   * Penalty for token presence\n   */\n  presencePenalty?: number;\n\n  /**\n   * Format for the response\n   */\n  responseFormat?: { type: 'text' | 'json_object' };\n\n  /**\n   * Random seed for deterministic results\n   */\n  seed?: number;\n\n  /**\n   * Sequences that stop generation\n   */\n  stop?: string | Array<string>;\n\n  /**\n   * Whether to stream responses\n   */\n  stream?: boolean;\n\n  /**\n   * Sampling temperature\n   */\n  temperature?: number;\n\n  /**\n   * Top-p sampling parameter\n   */\n  topProbability?: number;\n\n  /**\n   * Available tools for the model\n   */\n  tools?: Array<OpenAI.Chat.ChatCompletionTool>;\n\n  /**\n   * Tool selection behavior\n   */\n  toolChoice?:\n    | 'none'\n    | 'auto'\n    | { type: 'function'; function: { name: string } };\n\n  /**\n   * User identifier\n   */\n  user?: string;\n\n  /**\n   * Callback for handling streaming responses\n   */\n  onProgress?: (partialMessage: string) => void;\n}\n\n/**\n * Configuration options specific to OpenAI client\n */\nexport interface OpenAIClientOptions extends AIClientOptions {\n  /**\n   * OpenAI API key\n   */\n  apiKey?: string;\n\n  /**\n   * OpenAI API base URL\n   */\n  baseUrl?: string;\n}\n\n/**\n * Client implementation for the OpenAI API\n */\nexport class OpenAIClient extends AIClient {\n  /**\n   * OpenAI client instance\n   */\n  protected openai!: OpenAI;\n\n  /**\n   * Configuration options for this client\n   */\n  public options: OpenAIClientOptions;\n\n  /**\n   * Creates a new OpenAIClient\n   *\n   * @param options - OpenAI client configuration options\n   */\n  constructor(options: OpenAIClientOptions) {\n    super(options);\n    this.options = options;\n  }\n\n  /**\n   * Sends a message to OpenAI\n   *\n   * @param text - Message text\n   * @param options - Message options\n   * @returns Promise resolving to the OpenAI response\n   */\n  public async message(\n    text: string,\n    options: AIMessageOptions = { role: 'user' },\n  ) {\n    const response = await this.textCompletion(text, options);\n    return response;\n  }\n\n  /**\n   * Factory method to create and initialize an OpenAIClient\n   *\n   * @param options - OpenAI client configuration options\n   * @returns Promise resolving to an initialized OpenAIClient\n   */\n  public static async create(\n    options: OpenAIClientOptions,\n  ): Promise<OpenAIClient> {\n    const client = new OpenAIClient(options);\n    await client.initialize();\n    return client;\n  }\n\n  /**\n   * Initializes the OpenAI client\n   */\n  protected async initialize() {\n    this.openai = new OpenAI({\n      apiKey: this.options.apiKey,\n      baseURL: this.options.baseUrl,\n    });\n  }\n\n  /**\n   * Sends a text completion request to the OpenAI API\n   *\n   * @param message - The message to send\n   * @param options - Configuration options for the completion request\n   * @returns Promise resolving to the completion text\n   * @throws Error if the OpenAI API response is invalid\n   */\n  public async textCompletion(\n    message: string,\n    options: OpenAITextCompletionOptions = {},\n  ): Promise<string> {\n    const {\n      model = 'gpt-4o',\n      role = 'user',\n      history = [],\n      name: _name,\n      frequencyPenalty: frequency_penalty = 0,\n      logitBias: logit_bias,\n      logprobs = false,\n      topLogprobs: top_logprobs,\n      maxTokens: max_tokens,\n      n = 1,\n      presencePenalty: presence_penalty = 0,\n      responseFormat: response_format,\n      seed,\n      stop,\n      stream: _stream = false,\n      temperature = 1,\n      topProbability: top_p = 1,\n      tools,\n      toolChoice: tool_choice,\n      user,\n      onProgress,\n    } = options;\n\n    const messages = [\n      ...history,\n      {\n        role: role as OpenAI.Chat.ChatCompletionRole,\n        content: message,\n      } as OpenAI.Chat.ChatCompletionSystemMessageParam,\n    ];\n\n    if (onProgress) {\n      const stream = await this.openai.chat.completions.create({\n        model,\n        messages,\n        stream: true,\n        frequency_penalty,\n        logit_bias,\n        logprobs,\n        top_logprobs,\n        max_tokens,\n        n,\n        presence_penalty,\n        response_format,\n        seed,\n        stop,\n        temperature,\n        top_p,\n        tools,\n        tool_choice,\n        user,\n      });\n\n      let fullContent = '';\n      for await (const chunk of stream) {\n        const content = chunk.choices[0]?.delta?.content || '';\n        fullContent += content;\n        onProgress(content);\n      }\n\n      return fullContent;\n    }\n    const response = await this.openai.chat.completions.create({\n      model,\n      messages,\n      frequency_penalty,\n      logit_bias,\n      logprobs,\n      top_logprobs,\n      max_tokens,\n      n,\n      presence_penalty,\n      response_format,\n      seed,\n      stop,\n      stream: false,\n      temperature,\n      top_p,\n      tools,\n      tool_choice,\n      user,\n    });\n\n    const choice = response.choices[0];\n    if (!choice || !choice.message || !choice.message.content) {\n      throw new ApiError('Invalid response from OpenAI API: Missing content', {\n        model,\n        responseId: response.id,\n        choices: response.choices?.length || 0,\n        hasChoice: !!choice,\n        hasMessage: !!choice?.message,\n        hasContent: !!choice?.message?.content,\n      });\n    }\n    return choice.message.content;\n  }\n}\n\n/**\n * Options for getting an AI client with type information\n */\ntype GetAIClientOptions = AIClientOptions & {\n  type?: 'openai' | 'anthropic' | 'gemini' | 'bedrock' | 'huggingface';\n};\n\n/**\n * Factory function to create and initialize an appropriate AI client\n * Delegates to the modern getAI() factory for all provider types\n *\n * @param options - Client configuration options\n * @returns Promise resolving to an initialized AI client\n * @throws Error if client type is invalid\n */\nexport async function getAIClient(\n  options: GetAIClientOptions,\n): Promise<AIClient> {\n  // Delegate to modern factory for all providers\n  const { getAI } = await import('./factory.js');\n  return (await getAI(options as any)) as any;\n}\n","/**\n * Universal factory functions for creating AI provider instances\n * Works in both browser and Node.js environments\n */\n\nimport { ValidationError } from '@have/utils';\n\nimport type {\n  AIInterface,\n  AnthropicOptions,\n  BedrockOptions,\n  GeminiOptions,\n  GetAIOptions,\n  HuggingFaceOptions,\n  OpenAIOptions,\n} from './types';\n\n/**\n * Type guards for provider options\n */\n\n/**\n * Checks if the options are for OpenAI provider\n * @param options - The AI provider options to check\n * @returns True if options are for OpenAI provider (including default case)\n */\nfunction isOpenAIOptions(options: GetAIOptions): options is OpenAIOptions {\n  return !options.type || options.type === 'openai';\n}\n\n/**\n * Checks if the options are for Google Gemini provider\n * @param options - The AI provider options to check\n * @returns True if options are for Gemini provider\n */\nfunction isGeminiOptions(options: GetAIOptions): options is GeminiOptions {\n  return options.type === 'gemini';\n}\n\n/**\n * Checks if the options are for Anthropic Claude provider\n * @param options - The AI provider options to check\n * @returns True if options are for Anthropic provider\n */\nfunction isAnthropicOptions(\n  options: GetAIOptions,\n): options is AnthropicOptions {\n  return options.type === 'anthropic';\n}\n\n/**\n * Checks if the options are for Hugging Face provider\n * @param options - The AI provider options to check\n * @returns True if options are for Hugging Face provider\n */\nfunction isHuggingFaceOptions(\n  options: GetAIOptions,\n): options is HuggingFaceOptions {\n  return options.type === 'huggingface';\n}\n\n/**\n * Checks if the options are for AWS Bedrock provider\n * @param options - The AI provider options to check\n * @returns True if options are for Bedrock provider\n */\nfunction isBedrockOptions(options: GetAIOptions): options is BedrockOptions {\n  return options.type === 'bedrock';\n}\n\n/**\n * Creates an AI provider instance based on the provided options.\n * Universal version that works in both browser and Node.js environments.\n *\n * @param options - Configuration options for the AI provider. Must include provider type and credentials.\n * @returns Promise resolving to an AI provider instance that implements the AIInterface\n * @throws {ValidationError} When the provider type is unsupported or invalid\n *\n * @example\n * ```typescript\n * // Create OpenAI client\n * const openai = await getAI({\n *   type: 'openai',\n *   apiKey: process.env.OPENAI_API_KEY!,\n *   defaultModel: 'gpt-4o'\n * });\n *\n * // Create Anthropic client\n * const anthropic = await getAI({\n *   type: 'anthropic',\n *   apiKey: process.env.ANTHROPIC_API_KEY!,\n *   defaultModel: 'claude-3-5-sonnet-20241022'\n * });\n * ```\n */\nexport async function getAI(options: GetAIOptions): Promise<AIInterface> {\n  if (isOpenAIOptions(options)) {\n    const { OpenAIProvider } = await import('./providers/openai.js');\n    return new OpenAIProvider(options);\n  }\n\n  if (isGeminiOptions(options)) {\n    const { GeminiProvider } = await import('./providers/gemini.js');\n    return new GeminiProvider(options);\n  }\n\n  if (isAnthropicOptions(options)) {\n    const { AnthropicProvider } = await import('./providers/anthropic.js');\n    return new AnthropicProvider(options);\n  }\n\n  if (isHuggingFaceOptions(options)) {\n    const { HuggingFaceProvider } = await import('./providers/huggingface.js');\n    return new HuggingFaceProvider(options);\n  }\n\n  if (isBedrockOptions(options)) {\n    const { BedrockProvider } = await import('./providers/bedrock.js');\n    return new BedrockProvider(options);\n  }\n\n  throw new ValidationError('Unsupported AI provider type', {\n    supportedTypes: ['openai', 'gemini', 'anthropic', 'huggingface', 'bedrock'],\n    providedType: (options as any).type,\n  });\n}\n\n/**\n * Browser-compatible auto-detection of AI provider based on available credentials.\n * Does not rely on process.env, making it suitable for browser environments.\n *\n * @param options - Configuration options that may contain provider-specific credentials\n * @returns Promise resolving to an AI provider instance based on detected credentials\n * @throws {ValidationError} When no provider can be detected from the provided options\n *\n * @example\n * ```typescript\n * // Auto-detect OpenAI from apiKey\n * const client1 = await getAIAuto({\n *   apiKey: 'sk-...', // Detected as OpenAI\n *   defaultModel: 'gpt-4o'\n * });\n *\n * // Auto-detect Hugging Face from apiToken\n * const client2 = await getAIAuto({\n *   apiToken: 'hf_...', // Detected as Hugging Face\n *   model: 'microsoft/DialoGPT-medium'\n * });\n *\n * // Auto-detect AWS Bedrock from region and credentials\n * const client3 = await getAIAuto({\n *   region: 'us-east-1',\n *   credentials: {\n *     accessKeyId: 'AKIA...',\n *     secretAccessKey: 'xxx'\n *   }\n * });\n * ```\n */\nexport async function getAIAuto(\n  options: Record<string, any>,\n): Promise<AIInterface> {\n  // Auto-detect provider based on available credentials\n  if (options.apiKey && !options.type) {\n    // Default to OpenAI if apiKey is provided without explicit type\n    return getAI({ ...options, type: 'openai' } as OpenAIOptions);\n  }\n\n  if (options.apiToken) {\n    // Hugging Face uses apiToken\n    return getAI({ ...options, type: 'huggingface' } as HuggingFaceOptions);\n  }\n\n  if (options.region && options.credentials) {\n    // AWS Bedrock uses region and explicit credentials\n    return getAI({ ...options, type: 'bedrock' } as BedrockOptions);\n  }\n\n  if (options.projectId || options.anthropicVersion) {\n    // Try to detect based on provider-specific options\n    if (options.anthropicVersion) {\n      return getAI({ ...options, type: 'anthropic' } as AnthropicOptions);\n    }\n    if (options.projectId) {\n      return getAI({ ...options, type: 'gemini' } as GeminiOptions);\n    }\n  }\n\n  throw new ValidationError('Could not auto-detect AI provider from options', {\n    hint: 'Please specify a \"type\" field in options or provide provider-specific credentials',\n    supportedTypes: ['openai', 'gemini', 'anthropic', 'huggingface', 'bedrock'],\n    providedOptions: Object.keys(options),\n  });\n}\n","import type { AIThread } from './thread';\n\n/**\n * Options for creating AI messages\n */\nexport interface AIMessageOptions {\n  /**\n   * Role of the message sender\n   */\n  role?: 'user' | 'assistant' | 'system';\n\n  /**\n   * Format for the AI response\n   */\n  responseFormat?: { type: 'text' | 'json_object' };\n}\n\n/**\n * Represents a message in an AI conversation\n */\nexport class AIMessage {\n  /**\n   * Original options used to create this message\n   */\n  protected options;\n\n  /**\n   * Name of the message sender\n   */\n  public name: string;\n\n  /**\n   * Content of the message\n   */\n  public content: string;\n\n  /**\n   * Role of the message sender in the conversation\n   */\n  public role: 'user' | 'assistant' | 'system';\n\n  /**\n   * Creates a new AI message\n   *\n   * @param options - Message configuration\n   * @param options.role - Role of the message sender\n   * @param options.content - Content of the message\n   * @param options.name - Name of the message sender\n   */\n  constructor(options: {\n    role: 'user' | 'assistant' | 'system';\n    content: string;\n    name: string;\n  }) {\n    this.options = options;\n    this.role = options.role;\n    this.content = options.content;\n    this.name = options.name;\n  }\n\n  /**\n   * Factory method to create a new AI message\n   *\n   * @param options - Message configuration\n   * @param options.thread - Thread this message belongs to\n   * @param options.role - Role of the message sender\n   * @param options.content - Content of the message\n   * @param options.name - Name of the message sender\n   * @returns Promise resolving to a new AIMessage instance\n   */\n  static async create(options: {\n    thread: AIThread;\n    role: 'user' | 'assistant' | 'system';\n    content: string;\n    name: string;\n  }) {\n    return new AIMessage(options);\n  }\n}\n","import type OpenAI from 'openai';\nimport { AIClient, type AIClientOptions } from './client';\nimport { AIMessage } from './message';\n\n/**\n * Options for creating an AI conversation thread\n */\nexport interface AIThreadOptions {\n  /**\n   * Options for the AI client to use in this thread\n   */\n  ai: AIClientOptions;\n}\n\n/**\n * Represents a conversation thread with an AI model\n * Manages messages, references, and conversation state\n */\nexport class AIThread {\n  /**\n   * AI client instance for this thread\n   */\n  protected ai!: AIClient;\n\n  /**\n   * Options used to configure this thread\n   */\n  protected options: AIThreadOptions;\n\n  /**\n   * Messages in this conversation thread\n   */\n  private messages: AIMessage[] = [];\n\n  /**\n   * Reference materials to include in the conversation context\n   */\n  private references: { [name: string]: string } = {};\n\n  /**\n   * Creates a new AI thread\n   *\n   * @param options - Thread configuration options\n   */\n  constructor(options: AIThreadOptions) {\n    this.options = options;\n  }\n\n  /**\n   * Factory method to create and initialize a new AI thread\n   *\n   * @param options - Thread configuration options\n   * @returns Promise resolving to an initialized AIThread\n   */\n  static async create(options: AIThreadOptions) {\n    const thread = new AIThread(options);\n    await thread.initialize();\n    return thread; // No need to add system message here, do it in addSystem\n  }\n\n  /**\n   * Initializes the AI client for this thread\n   */\n  public async initialize() {\n    this.ai = await AIClient.create(this.options.ai);\n  }\n\n  /**\n   * Adds a system message to the conversation\n   *\n   * @param prompt - System message content\n   * @returns Promise resolving to the created AIMessage\n   */\n  public async addSystem(prompt: string) {\n    const message = await AIMessage.create({\n      thread: this,\n      role: 'system',\n      name: 'system',\n      content: prompt,\n    });\n\n    this.messages.push(message);\n    return message;\n  }\n\n  /**\n   * Adds a message to the conversation\n   *\n   * @param options - Message options\n   * @param options.role - Role of the message sender\n   * @param options.name - Optional name of the message sender\n   * @param options.content - Content of the message\n   * @returns Promise resolving to the created AIMessage\n   */\n  public async add(options: {\n    role: 'user' | 'assistant' | 'system';\n    name?: string;\n    content: string;\n  }) {\n    const message = await AIMessage.create({\n      thread: this,\n      role: options.role,\n      name: options.name || options.role, // Default name to role if not provided\n      content: options.content,\n    });\n\n    this.messages.push(message);\n    return message;\n  }\n\n  /**\n   * Gets all messages in this thread\n   *\n   * @returns Array of AIMessage objects\n   */\n  public get(): AIMessage[] {\n    return this.messages;\n  }\n\n  /**\n   * Adds a reference to be included in the conversation context\n   *\n   * @param name - Name of the reference\n   * @param body - Content of the reference\n   */\n  public addReference(name: string, body: string): void {\n    this.references[name] = body;\n  }\n\n  /**\n   * Assembles the conversation history for sending to the AI\n   * Properly orders system message, references, and conversation messages\n   *\n   * @returns Array of message parameters formatted for the OpenAI API\n   */\n  public assembleHistory(): OpenAI.Chat.ChatCompletionMessageParam[] {\n    const history: OpenAI.Chat.ChatCompletionMessageParam[] = [];\n\n    // Add system message first\n    const systemMessage = this.messages.find((m) => m.role === 'system');\n    if (systemMessage) {\n      history.push({\n        role: systemMessage.role,\n        content: systemMessage.content,\n      });\n    }\n\n    // Add references as user messages (before other user/assistant messages)\n    for (const name in this.references) {\n      history.push({\n        role: 'user',\n        content: `Reference - ${name}:\\n${this.references[name]}`,\n      });\n    }\n\n    // Add other messages\n    this.messages\n      .filter((m) => m.role !== 'system')\n      .forEach((message) => {\n        history.push({ role: message.role, content: message.content });\n      });\n\n    return history;\n  }\n\n  /**\n   * Sends a prompt to the AI and gets a response\n   *\n   * @param prompt - Prompt message to send\n   * @param options - Options for the AI response\n   * @param options.responseFormat - Format for the AI to respond with\n   * @returns Promise resolving to the AI response\n   */\n  public async do(\n    prompt: string,\n    options: {\n      responseFormat?: 'html' | 'text' | 'json';\n    } = {\n      responseFormat: 'text',\n    },\n  ) {\n    const { responseFormat } = options;\n    const history = this.assembleHistory();\n\n    // Get completion from AI with assembled history\n    const response = await this.ai.textCompletion(prompt, {\n      history,\n      responseFormat: {\n        type: responseFormat === 'json' ? 'json_object' : 'text',\n      },\n    });\n    return response;\n  }\n}\n","/**\n * Core types and interfaces for the AI library\n */\n\n/**\n * AI message structure for chat interactions\n */\nexport interface AIMessage {\n  /**\n   * Role of the message sender\n   */\n  role: 'system' | 'user' | 'assistant' | 'function' | 'tool';\n\n  /**\n   * Content of the message\n   */\n  content: string;\n\n  /**\n   * Optional name for the message sender\n   */\n  name?: string;\n\n  /**\n   * Optional tool calls\n   */\n  tool_calls?: Array<{\n    id: string;\n    type: 'function';\n    function: {\n      name: string;\n      arguments: string;\n    };\n  }>;\n}\n\n/**\n * Options for chat completion requests\n */\nexport interface ChatOptions {\n  /**\n   * Model to use for completion\n   */\n  model?: string;\n\n  /**\n   * Maximum number of tokens to generate\n   */\n  maxTokens?: number;\n\n  /**\n   * Sampling temperature (0-2)\n   */\n  temperature?: number;\n\n  /**\n   * Top-p sampling parameter\n   */\n  topP?: number;\n\n  /**\n   * Number of completions to generate\n   */\n  n?: number;\n\n  /**\n   * Sequences that stop generation\n   */\n  stop?: string | string[];\n\n  /**\n   * Whether to stream the response\n   */\n  stream?: boolean;\n\n  /**\n   * Penalty for frequency of tokens\n   */\n  frequencyPenalty?: number;\n\n  /**\n   * Penalty for presence of tokens\n   */\n  presencePenalty?: number;\n\n  /**\n   * User identifier for monitoring\n   */\n  user?: string;\n\n  /**\n   * Available tools/functions\n   */\n  tools?: AITool[];\n\n  /**\n   * Tool choice behavior\n   */\n  toolChoice?:\n    | 'auto'\n    | 'none'\n    | { type: 'function'; function: { name: string } };\n\n  /**\n   * Response format specification\n   */\n  responseFormat?: { type: 'text' | 'json_object' };\n\n  /**\n   * Random seed for deterministic results\n   */\n  seed?: number;\n\n  /**\n   * Callback for streaming responses\n   */\n  onProgress?: (chunk: string) => void;\n}\n\n/**\n * Options for text completion requests (non-chat models)\n */\nexport interface CompletionOptions {\n  /**\n   * Model to use for completion\n   */\n  model?: string;\n\n  /**\n   * Maximum number of tokens to generate\n   */\n  maxTokens?: number;\n\n  /**\n   * Sampling temperature\n   */\n  temperature?: number;\n\n  /**\n   * Top-p sampling parameter\n   */\n  topP?: number;\n\n  /**\n   * Number of completions to generate\n   */\n  n?: number;\n\n  /**\n   * Sequences that stop generation\n   */\n  stop?: string | string[];\n\n  /**\n   * Whether to stream the response\n   */\n  stream?: boolean;\n\n  /**\n   * Callback for streaming responses\n   */\n  onProgress?: (chunk: string) => void;\n}\n\n/**\n * Options for embedding generation\n */\nexport interface EmbeddingOptions {\n  /**\n   * Model to use for embeddings\n   */\n  model?: string;\n\n  /**\n   * User identifier for monitoring\n   */\n  user?: string;\n\n  /**\n   * Encoding format for embeddings\n   */\n  encodingFormat?: 'float' | 'base64';\n\n  /**\n   * Number of dimensions for the embedding\n   */\n  dimensions?: number;\n}\n\n/**\n * Tool/function definition for AI models\n */\nexport interface AITool {\n  /**\n   * Type of tool\n   */\n  type: 'function';\n\n  /**\n   * Function definition\n   */\n  function: {\n    /**\n     * Function name\n     */\n    name: string;\n\n    /**\n     * Function description\n     */\n    description?: string;\n\n    /**\n     * JSON schema for function parameters\n     */\n    parameters?: Record<string, any>;\n  };\n}\n\n/**\n * Model information structure\n */\nexport interface AIModel {\n  /**\n   * Model identifier\n   */\n  id: string;\n\n  /**\n   * Human-readable model name\n   */\n  name: string;\n\n  /**\n   * Model description\n   */\n  description?: string;\n\n  /**\n   * Maximum context length in tokens\n   */\n  contextLength: number;\n\n  /**\n   * Supported capabilities\n   */\n  capabilities: string[];\n\n  /**\n   * Whether the model supports function calling\n   */\n  supportsFunctions: boolean;\n\n  /**\n   * Whether the model supports vision/multimodal input\n   */\n  supportsVision: boolean;\n\n  /**\n   * Cost per input token (if available)\n   */\n  inputCostPer1k?: number;\n\n  /**\n   * Cost per output token (if available)\n   */\n  outputCostPer1k?: number;\n}\n\n/**\n * AI provider capabilities\n */\nexport interface AICapabilities {\n  /**\n   * Whether the provider supports chat completions\n   */\n  chat: boolean;\n\n  /**\n   * Whether the provider supports text completions\n   */\n  completion: boolean;\n\n  /**\n   * Whether the provider supports embeddings\n   */\n  embeddings: boolean;\n\n  /**\n   * Whether the provider supports streaming\n   */\n  streaming: boolean;\n\n  /**\n   * Whether the provider supports function calling\n   */\n  functions: boolean;\n\n  /**\n   * Whether the provider supports vision/multimodal\n   */\n  vision: boolean;\n\n  /**\n   * Whether the provider supports fine-tuning\n   */\n  fineTuning: boolean;\n\n  /**\n   * Maximum context length supported\n   */\n  maxContextLength: number;\n\n  /**\n   * Supported operations\n   */\n  supportedOperations: string[];\n}\n\n/**\n * Token usage information\n */\nexport interface TokenUsage {\n  /**\n   * Number of prompt tokens\n   */\n  promptTokens: number;\n\n  /**\n   * Number of completion tokens\n   */\n  completionTokens: number;\n\n  /**\n   * Total tokens used\n   */\n  totalTokens: number;\n}\n\n/**\n * AI response structure\n */\nexport interface AIResponse {\n  /**\n   * Generated content\n   */\n  content: string;\n\n  /**\n   * Token usage information\n   */\n  usage?: TokenUsage;\n\n  /**\n   * Model used for generation\n   */\n  model?: string;\n\n  /**\n   * Finish reason\n   */\n  finishReason?: 'stop' | 'length' | 'tool_calls' | 'content_filter';\n\n  /**\n   * Tool calls made by the model\n   */\n  toolCalls?: Array<{\n    id: string;\n    type: 'function';\n    function: {\n      name: string;\n      arguments: string;\n    };\n  }>;\n}\n\n/**\n * Embedding response structure\n */\nexport interface EmbeddingResponse {\n  /**\n   * Generated embeddings\n   */\n  embeddings: number[][];\n\n  /**\n   * Token usage information\n   */\n  usage?: TokenUsage;\n\n  /**\n   * Model used for embeddings\n   */\n  model?: string;\n}\n\n/**\n * Core AI interface that all providers must implement\n */\nexport interface AIInterface {\n  /**\n   * Generate chat completion\n   */\n  chat(messages: AIMessage[], options?: ChatOptions): Promise<AIResponse>;\n\n  /**\n   * Generate text completion (for non-chat models)\n   */\n  complete(prompt: string, options?: CompletionOptions): Promise<AIResponse>;\n\n  /**\n   * Generate embeddings for text\n   */\n  embed(\n    text: string | string[],\n    options?: EmbeddingOptions,\n  ): Promise<EmbeddingResponse>;\n\n  /**\n   * Stream chat completion\n   */\n  stream(messages: AIMessage[], options?: ChatOptions): AsyncIterable<string>;\n\n  /**\n   * Count tokens in text\n   */\n  countTokens(text: string): Promise<number>;\n\n  /**\n   * Get available models\n   */\n  getModels(): Promise<AIModel[]>;\n\n  /**\n   * Get provider capabilities\n   */\n  getCapabilities(): Promise<AICapabilities>;\n}\n\n/**\n * Base configuration options for all providers\n */\nexport interface BaseAIOptions {\n  /**\n   * API timeout in milliseconds\n   */\n  timeout?: number;\n\n  /**\n   * Maximum number of retries\n   */\n  maxRetries?: number;\n\n  /**\n   * Custom headers\n   */\n  headers?: Record<string, string>;\n\n  /**\n   * Default model to use\n   */\n  defaultModel?: string;\n}\n\n/**\n * OpenAI provider options\n */\nexport interface OpenAIOptions extends BaseAIOptions {\n  type?: 'openai';\n  apiKey: string;\n  baseUrl?: string;\n  organization?: string;\n}\n\n/**\n * Gemini provider options\n */\nexport interface GeminiOptions extends BaseAIOptions {\n  type: 'gemini';\n  apiKey: string;\n  baseUrl?: string;\n  projectId?: string;\n  location?: string;\n}\n\n/**\n * Anthropic provider options\n */\nexport interface AnthropicOptions extends BaseAIOptions {\n  type: 'anthropic';\n  apiKey: string;\n  baseUrl?: string;\n  anthropicVersion?: string;\n}\n\n/**\n * Hugging Face provider options\n */\nexport interface HuggingFaceOptions extends BaseAIOptions {\n  type: 'huggingface';\n  apiToken: string;\n  endpoint?: string;\n  model?: string;\n  useCache?: boolean;\n  waitForModel?: boolean;\n}\n\n/**\n * AWS Bedrock provider options\n */\nexport interface BedrockOptions extends BaseAIOptions {\n  type: 'bedrock';\n  region: string;\n  credentials?: {\n    accessKeyId: string;\n    secretAccessKey: string;\n    sessionToken?: string;\n  };\n  endpoint?: string;\n}\n\n/**\n * Union type for all provider options\n */\nexport type GetAIOptions =\n  | OpenAIOptions\n  | GeminiOptions\n  | AnthropicOptions\n  | HuggingFaceOptions\n  | BedrockOptions;\n\n/**\n * Error types for AI operations\n */\nexport class AIError extends Error {\n  constructor(\n    message: string,\n    public code: string,\n    public provider?: string,\n    public model?: string,\n  ) {\n    super(message);\n    this.name = 'AIError';\n  }\n}\n\nexport class AuthenticationError extends AIError {\n  constructor(provider?: string) {\n    super('Authentication failed', 'AUTH_ERROR', provider);\n    this.name = 'AuthenticationError';\n  }\n}\n\nexport class RateLimitError extends AIError {\n  constructor(provider?: string, retryAfter?: number) {\n    super(\n      `Rate limit exceeded${retryAfter ? `, retry after ${retryAfter}s` : ''}`,\n      'RATE_LIMIT',\n      provider,\n    );\n    this.name = 'RateLimitError';\n  }\n}\n\nexport class ModelNotFoundError extends AIError {\n  constructor(model: string, provider?: string) {\n    super(`Model not found: ${model}`, 'MODEL_NOT_FOUND', provider, model);\n    this.name = 'ModelNotFoundError';\n  }\n}\n\nexport class ContextLengthError extends AIError {\n  constructor(provider?: string, model?: string) {\n    super(\n      'Input exceeds maximum context length',\n      'CONTEXT_LENGTH_EXCEEDED',\n      provider,\n      model,\n    );\n    this.name = 'ContextLengthError';\n  }\n}\n\nexport class ContentFilterError extends AIError {\n  constructor(provider?: string, model?: string) {\n    super(\n      'Content filtered by safety systems',\n      'CONTENT_FILTERED',\n      provider,\n      model,\n    );\n    this.name = 'ContentFilterError';\n  }\n}\n"],"names":["getAI"],"mappings":";;AAgEA,SAAS,sBACP,SACgC;AAChC,SAAO,QAAQ,SAAS,YAAY,YAAY,WAAW,CAAC,CAAC,QAAQ;AACvE;AAQA,SAAS,mBAAmB,OAA+B;AACzD,SAAO,iBAAiB;AAC1B;AA4HO,MAAM,SAAS;AAAA;AAAA;AAAA;AAAA,EAIb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOP,YAAY,SAA0B;AACpC,SAAK,UAAU;AAAA,EACjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAUA,MAAa,QACX,OACA,WAAoC,EAAE,MAAM,UAC5C;AACA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,aAAoB,OAClB,SACkC;AAElC,QAAI,mBAAmB,OAAO,GAAG;AAC/B,aAAO;AAAA,IACT;AAGA,UAAM,gBAAgB;AAEtB,QAAI,sBAAsB,aAAa,GAAG;AACxC,aAAO,aAAa,OAAO,aAAa;AAAA,IAC1C;AAGA,UAAM,eAAgB,cAAsB;AAC5C,QAAI,gBAAgB,iBAAiB,UAAU;AAC7C,YAAM,EAAE,OAAAA,OAAA,IAAU,MAAM,QAAA,QAAA,EAAA,KAAA,MAAA,OAAA;AACxB,aAAQ,MAAMA,OAAM,aAAoB;AAAA,IAC1C;AAGA,QAAI,iBAAiB,UAAU;AAC7B,YAAM,IAAI;AAAA,QACR;AAAA,QACA;AAAA,UACE,gBAAgB;AAAA,YACd;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,UAAA;AAAA,UAEF;AAAA,UACA,MAAM;AAAA,QAAA;AAAA,MACR;AAAA,IAEJ;AAEA,UAAM,IAAI,gBAAgB,iCAAiC;AAAA,MACzD,gBAAgB;AAAA,QACd;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MAAA;AAAA,MAEF;AAAA,IAAA,CACD;AAAA,EACH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAUO,eACL,MACA,UAAmC;AAAA,IACjC,MAAM;AAAA,EAAA,GAER;AACA,WAAO,KAAK,QAAQ,MAAM,OAAO;AAAA,EACnC;AACF;AAQA,eAAsB,UAAU,SAG7B;AACD,SAAO,IAAI,OAAO;AAAA,IAChB,QAAQ,QAAQ;AAAA,IAChB,SAAS,QAAQ;AAAA,EAAA,CAClB;AACH;AA0IO,MAAM,qBAAqB,SAAS;AAAA;AAAA;AAAA;AAAA,EAI/B;AAAA;AAAA;AAAA;AAAA,EAKH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOP,YAAY,SAA8B;AACxC,UAAM,OAAO;AACb,SAAK,UAAU;AAAA,EACjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAa,QACX,MACA,UAA4B,EAAE,MAAM,UACpC;AACA,UAAM,WAAW,MAAM,KAAK,eAAe,MAAM,OAAO;AACxD,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,aAAoB,OAClB,SACuB;AACvB,UAAM,SAAS,IAAI,aAAa,OAAO;AACvC,UAAM,OAAO,WAAA;AACb,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAgB,aAAa;AAC3B,SAAK,SAAS,IAAI,OAAO;AAAA,MACvB,QAAQ,KAAK,QAAQ;AAAA,MACrB,SAAS,KAAK,QAAQ;AAAA,IAAA,CACvB;AAAA,EACH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAUA,MAAa,eACX,SACA,UAAuC,IACtB;AACjB,UAAM;AAAA,MACJ,QAAQ;AAAA,MACR,OAAO;AAAA,MACP,UAAU,CAAA;AAAA,MACV,MAAM;AAAA,MACN,kBAAkB,oBAAoB;AAAA,MACtC,WAAW;AAAA,MACX,WAAW;AAAA,MACX,aAAa;AAAA,MACb,WAAW;AAAA,MACX,IAAI;AAAA,MACJ,iBAAiB,mBAAmB;AAAA,MACpC,gBAAgB;AAAA,MAChB;AAAA,MACA;AAAA,MACA,QAAQ,UAAU;AAAA,MAClB,cAAc;AAAA,MACd,gBAAgB,QAAQ;AAAA,MACxB;AAAA,MACA,YAAY;AAAA,MACZ;AAAA,MACA;AAAA,IAAA,IACE;AAEJ,UAAM,WAAW;AAAA,MACf,GAAG;AAAA,MACH;AAAA,QACE;AAAA,QACA,SAAS;AAAA,MAAA;AAAA,IACX;AAGF,QAAI,YAAY;AACd,YAAM,SAAS,MAAM,KAAK,OAAO,KAAK,YAAY,OAAO;AAAA,QACvD;AAAA,QACA;AAAA,QACA,QAAQ;AAAA,QACR;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MAAA,CACD;AAED,UAAI,cAAc;AAClB,uBAAiB,SAAS,QAAQ;AAChC,cAAM,UAAU,MAAM,QAAQ,CAAC,GAAG,OAAO,WAAW;AACpD,uBAAe;AACf,mBAAW,OAAO;AAAA,MACpB;AAEA,aAAO;AAAA,IACT;AACA,UAAM,WAAW,MAAM,KAAK,OAAO,KAAK,YAAY,OAAO;AAAA,MACzD;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,QAAQ;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IAAA,CACD;AAED,UAAM,SAAS,SAAS,QAAQ,CAAC;AACjC,QAAI,CAAC,UAAU,CAAC,OAAO,WAAW,CAAC,OAAO,QAAQ,SAAS;AACzD,YAAM,IAAI,SAAS,qDAAqD;AAAA,QACtE;AAAA,QACA,YAAY,SAAS;AAAA,QACrB,SAAS,SAAS,SAAS,UAAU;AAAA,QACrC,WAAW,CAAC,CAAC;AAAA,QACb,YAAY,CAAC,CAAC,QAAQ;AAAA,QACtB,YAAY,CAAC,CAAC,QAAQ,SAAS;AAAA,MAAA,CAChC;AAAA,IACH;AACA,WAAO,OAAO,QAAQ;AAAA,EACxB;AACF;AAiBA,eAAsB,YACpB,SACmB;AAEnB,QAAM,EAAE,OAAAA,OAAA,IAAU,MAAM,QAAA,QAAA,EAAA,KAAA,MAAA,OAAA;AACxB,SAAQ,MAAMA,OAAM,OAAc;AACpC;ACnnBA,SAAS,gBAAgB,SAAiD;AACxE,SAAO,CAAC,QAAQ,QAAQ,QAAQ,SAAS;AAC3C;AAOA,SAAS,gBAAgB,SAAiD;AACxE,SAAO,QAAQ,SAAS;AAC1B;AAOA,SAAS,mBACP,SAC6B;AAC7B,SAAO,QAAQ,SAAS;AAC1B;AAOA,SAAS,qBACP,SAC+B;AAC/B,SAAO,QAAQ,SAAS;AAC1B;AAOA,SAAS,iBAAiB,SAAkD;AAC1E,SAAO,QAAQ,SAAS;AAC1B;AA2BA,eAAsB,MAAM,SAA6C;AACvE,MAAI,gBAAgB,OAAO,GAAG;AAC5B,UAAM,EAAE,eAAA,IAAmB,MAAM,OAAO,6BAAuB;AAC/D,WAAO,IAAI,eAAe,OAAO;AAAA,EACnC;AAEA,MAAI,gBAAgB,OAAO,GAAG;AAC5B,UAAM,EAAE,eAAA,IAAmB,MAAM,OAAO,6BAAuB;AAC/D,WAAO,IAAI,eAAe,OAAO;AAAA,EACnC;AAEA,MAAI,mBAAmB,OAAO,GAAG;AAC/B,UAAM,EAAE,kBAAA,IAAsB,MAAM,OAAO,gCAA0B;AACrE,WAAO,IAAI,kBAAkB,OAAO;AAAA,EACtC;AAEA,MAAI,qBAAqB,OAAO,GAAG;AACjC,UAAM,EAAE,oBAAA,IAAwB,MAAM,OAAO,kCAA4B;AACzE,WAAO,IAAI,oBAAoB,OAAO;AAAA,EACxC;AAEA,MAAI,iBAAiB,OAAO,GAAG;AAC7B,UAAM,EAAE,gBAAA,IAAoB,MAAM,OAAO,8BAAwB;AACjE,WAAO,IAAI,gBAAgB,OAAO;AAAA,EACpC;AAEA,QAAM,IAAI,gBAAgB,gCAAgC;AAAA,IACxD,gBAAgB,CAAC,UAAU,UAAU,aAAa,eAAe,SAAS;AAAA,IAC1E,cAAe,QAAgB;AAAA,EAAA,CAChC;AACH;AAkCA,eAAsB,UACpB,SACsB;AAEtB,MAAI,QAAQ,UAAU,CAAC,QAAQ,MAAM;AAEnC,WAAO,MAAM,EAAE,GAAG,SAAS,MAAM,UAA2B;AAAA,EAC9D;AAEA,MAAI,QAAQ,UAAU;AAEpB,WAAO,MAAM,EAAE,GAAG,SAAS,MAAM,eAAqC;AAAA,EACxE;AAEA,MAAI,QAAQ,UAAU,QAAQ,aAAa;AAEzC,WAAO,MAAM,EAAE,GAAG,SAAS,MAAM,WAA6B;AAAA,EAChE;AAEA,MAAI,QAAQ,aAAa,QAAQ,kBAAkB;AAEjD,QAAI,QAAQ,kBAAkB;AAC5B,aAAO,MAAM,EAAE,GAAG,SAAS,MAAM,aAAiC;AAAA,IACpE;AACA,QAAI,QAAQ,WAAW;AACrB,aAAO,MAAM,EAAE,GAAG,SAAS,MAAM,UAA2B;AAAA,IAC9D;AAAA,EACF;AAEA,QAAM,IAAI,gBAAgB,kDAAkD;AAAA,IAC1E,MAAM;AAAA,IACN,gBAAgB,CAAC,UAAU,UAAU,aAAa,eAAe,SAAS;AAAA,IAC1E,iBAAiB,OAAO,KAAK,OAAO;AAAA,EAAA,CACrC;AACH;;;;;;AC7KO,MAAM,UAAU;AAAA;AAAA;AAAA;AAAA,EAIX;AAAA;AAAA;AAAA;AAAA,EAKH;AAAA;AAAA;AAAA;AAAA,EAKA;AAAA;AAAA;AAAA;AAAA,EAKA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAUP,YAAY,SAIT;AACD,SAAK,UAAU;AACf,SAAK,OAAO,QAAQ;AACpB,SAAK,UAAU,QAAQ;AACvB,SAAK,OAAO,QAAQ;AAAA,EACtB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAYA,aAAa,OAAO,SAKjB;AACD,WAAO,IAAI,UAAU,OAAO;AAAA,EAC9B;AACF;AC5DO,MAAM,SAAS;AAAA;AAAA;AAAA;AAAA,EAIV;AAAA;AAAA;AAAA;AAAA,EAKA;AAAA;AAAA;AAAA;AAAA,EAKF,WAAwB,CAAA;AAAA;AAAA;AAAA;AAAA,EAKxB,aAAyC,CAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOjD,YAAY,SAA0B;AACpC,SAAK,UAAU;AAAA,EACjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,aAAa,OAAO,SAA0B;AAC5C,UAAM,SAAS,IAAI,SAAS,OAAO;AACnC,UAAM,OAAO,WAAA;AACb,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,MAAa,aAAa;AACxB,SAAK,KAAK,MAAM,SAAS,OAAO,KAAK,QAAQ,EAAE;AAAA,EACjD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,MAAa,UAAU,QAAgB;AACrC,UAAM,UAAU,MAAM,UAAU,OAAO;AAAA,MACrC,QAAQ;AAAA,MACR,MAAM;AAAA,MACN,MAAM;AAAA,MACN,SAAS;AAAA,IAAA,CACV;AAED,SAAK,SAAS,KAAK,OAAO;AAC1B,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAWA,MAAa,IAAI,SAId;AACD,UAAM,UAAU,MAAM,UAAU,OAAO;AAAA,MACrC,QAAQ;AAAA,MACR,MAAM,QAAQ;AAAA,MACd,MAAM,QAAQ,QAAQ,QAAQ;AAAA;AAAA,MAC9B,SAAS,QAAQ;AAAA,IAAA,CAClB;AAED,SAAK,SAAS,KAAK,OAAO;AAC1B,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOO,MAAmB;AACxB,WAAO,KAAK;AAAA,EACd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQO,aAAa,MAAc,MAAoB;AACpD,SAAK,WAAW,IAAI,IAAI;AAAA,EAC1B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQO,kBAA4D;AACjE,UAAM,UAAoD,CAAA;AAG1D,UAAM,gBAAgB,KAAK,SAAS,KAAK,CAAC,MAAM,EAAE,SAAS,QAAQ;AACnE,QAAI,eAAe;AACjB,cAAQ,KAAK;AAAA,QACX,MAAM,cAAc;AAAA,QACpB,SAAS,cAAc;AAAA,MAAA,CACxB;AAAA,IACH;AAGA,eAAW,QAAQ,KAAK,YAAY;AAClC,cAAQ,KAAK;AAAA,QACX,MAAM;AAAA,QACN,SAAS,eAAe,IAAI;AAAA,EAAM,KAAK,WAAW,IAAI,CAAC;AAAA,MAAA,CACxD;AAAA,IACH;AAGA,SAAK,SACF,OAAO,CAAC,MAAM,EAAE,SAAS,QAAQ,EACjC,QAAQ,CAAC,YAAY;AACpB,cAAQ,KAAK,EAAE,MAAM,QAAQ,MAAM,SAAS,QAAQ,SAAS;AAAA,IAC/D,CAAC;AAEH,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAUA,MAAa,GACX,QACA,UAEI;AAAA,IACF,gBAAgB;AAAA,EAAA,GAElB;AACA,UAAM,EAAE,mBAAmB;AAC3B,UAAM,UAAU,KAAK,gBAAA;AAGrB,UAAM,WAAW,MAAM,KAAK,GAAG,eAAe,QAAQ;AAAA,MACpD;AAAA,MACA,gBAAgB;AAAA,QACd,MAAM,mBAAmB,SAAS,gBAAgB;AAAA,MAAA;AAAA,IACpD,CACD;AACD,WAAO;AAAA,EACT;AACF;ACqVO,MAAM,gBAAgB,MAAM;AAAA,EACjC,YACE,SACO,MACA,UACA,OACP;AACA,UAAM,OAAO;AAJN,SAAA,OAAA;AACA,SAAA,WAAA;AACA,SAAA,QAAA;AAGP,SAAK,OAAO;AAAA,EACd;AACF;AAEO,MAAM,4BAA4B,QAAQ;AAAA,EAC/C,YAAY,UAAmB;AAC7B,UAAM,yBAAyB,cAAc,QAAQ;AACrD,SAAK,OAAO;AAAA,EACd;AACF;AAEO,MAAM,uBAAuB,QAAQ;AAAA,EAC1C,YAAY,UAAmB,YAAqB;AAClD;AAAA,MACE,sBAAsB,aAAa,iBAAiB,UAAU,MAAM,EAAE;AAAA,MACtE;AAAA,MACA;AAAA,IAAA;AAEF,SAAK,OAAO;AAAA,EACd;AACF;AAEO,MAAM,2BAA2B,QAAQ;AAAA,EAC9C,YAAY,OAAe,UAAmB;AAC5C,UAAM,oBAAoB,KAAK,IAAI,mBAAmB,UAAU,KAAK;AACrE,SAAK,OAAO;AAAA,EACd;AACF;AAEO,MAAM,2BAA2B,QAAQ;AAAA,EAC9C,YAAY,UAAmB,OAAgB;AAC7C;AAAA,MACE;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IAAA;AAEF,SAAK,OAAO;AAAA,EACd;AACF;AAEO,MAAM,2BAA2B,QAAQ;AAAA,EAC9C,YAAY,UAAmB,OAAgB;AAC7C;AAAA,MACE;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IAAA;AAEF,SAAK,OAAO;AAAA,EACd;AACF;"}