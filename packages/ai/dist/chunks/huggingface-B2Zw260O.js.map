{"version":3,"file":"huggingface-B2Zw260O.js","sources":["../../src/shared/providers/huggingface.ts"],"sourcesContent":["/**\n * Hugging Face provider implementation\n */\n\nimport type {\n  AICapabilities,\n  AIInterface,\n  AIMessage,\n  AIModel,\n  AIResponse,\n  ChatOptions,\n  CompletionOptions,\n  EmbeddingOptions,\n  EmbeddingResponse,\n  HuggingFaceOptions,\n} from '../types';\nimport {\n  AIError,\n  AuthenticationError,\n  ContextLengthError,\n  ModelNotFoundError,\n  RateLimitError,\n} from '../types';\n\nexport class HuggingFaceProvider implements AIInterface {\n  private options: HuggingFaceOptions;\n  private baseUrl: string;\n\n  constructor(options: HuggingFaceOptions) {\n    this.options = {\n      defaultModel: 'microsoft/DialoGPT-medium',\n      useCache: true,\n      waitForModel: true,\n      ...options,\n    };\n\n    this.baseUrl =\n      this.options.endpoint || 'https://api-inference.huggingface.co';\n  }\n\n  async chat(\n    messages: AIMessage[],\n    options: ChatOptions = {},\n  ): Promise<AIResponse> {\n    try {\n      // Convert messages to a single prompt for text generation models\n      const prompt = this.messagesToPrompt(messages);\n\n      const response = await this.makeRequest(\n        `/models/${options.model || this.options.model || this.options.defaultModel}`,\n        {\n          inputs: prompt,\n          parameters: {\n            max_new_tokens: options.maxTokens || 512,\n            temperature: options.temperature || 1.0,\n            top_p: options.topP || 1.0,\n            do_sample:\n              (options.temperature && options.temperature > 0) || false,\n            stop_sequences: Array.isArray(options.stop)\n              ? options.stop\n              : options.stop\n                ? [options.stop]\n                : undefined,\n          },\n          options: {\n            use_cache: this.options.useCache,\n            wait_for_model: this.options.waitForModel,\n          },\n        },\n      );\n\n      if (Array.isArray(response) && response[0]?.generated_text) {\n        const generatedText = response[0].generated_text;\n        // Remove the input prompt from the response\n        const content = generatedText.replace(prompt, '').trim();\n\n        return {\n          content,\n          model:\n            options.model || this.options.model || this.options.defaultModel,\n          finishReason: 'stop',\n        };\n      }\n\n      throw new AIError(\n        'Invalid response format from Hugging Face',\n        'INVALID_RESPONSE',\n        'huggingface',\n      );\n    } catch (error) {\n      throw this.mapError(error);\n    }\n  }\n\n  async complete(\n    prompt: string,\n    options: CompletionOptions = {},\n  ): Promise<AIResponse> {\n    return this.chat([{ role: 'user', content: prompt }], {\n      model: options.model,\n      maxTokens: options.maxTokens,\n      temperature: options.temperature,\n      topP: options.topP,\n      n: options.n,\n      stop: options.stop,\n      stream: options.stream,\n      onProgress: options.onProgress,\n    });\n  }\n\n  async embed(\n    text: string | string[],\n    options: EmbeddingOptions = {},\n  ): Promise<EmbeddingResponse> {\n    try {\n      const input = Array.isArray(text) ? text : [text];\n      const model = options.model || 'sentence-transformers/all-MiniLM-L6-v2';\n\n      const response = await this.makeRequest(`/models/${model}`, {\n        inputs: input,\n        options: {\n          use_cache: this.options.useCache,\n          wait_for_model: this.options.waitForModel,\n        },\n      });\n\n      // Handle different response formats from different embedding models\n      let embeddings: number[][];\n      if (Array.isArray(response) && Array.isArray(response[0])) {\n        // Direct array of embeddings\n        embeddings = Array.isArray(text) ? response : [response[0]];\n      } else if (\n        response &&\n        typeof response === 'object' &&\n        response.embeddings\n      ) {\n        // Response with embeddings property\n        embeddings = response.embeddings;\n      } else {\n        throw new AIError(\n          'Invalid embedding response format',\n          'INVALID_RESPONSE',\n          'huggingface',\n        );\n      }\n\n      return {\n        embeddings,\n        model,\n      };\n    } catch (error) {\n      throw this.mapError(error);\n    }\n  }\n\n  async *stream(\n    messages: AIMessage[],\n    options: ChatOptions = {},\n  ): AsyncIterable<string> {\n    // Hugging Face Inference API doesn't support streaming for most models\n    // Fall back to regular completion and yield the result\n    const response = await this.chat(messages, options);\n\n    // Simulate streaming by yielding chunks\n    const content = response.content;\n    const chunkSize = 10;\n\n    for (let i = 0; i < content.length; i += chunkSize) {\n      const chunk = content.slice(i, i + chunkSize);\n      if (options.onProgress) {\n        options.onProgress(chunk);\n      }\n      yield chunk;\n\n      // Add small delay to simulate streaming\n      await new Promise((resolve) => setTimeout(resolve, 50));\n    }\n  }\n\n  async countTokens(text: string): Promise<number> {\n    // Approximation - Hugging Face models use different tokenizers\n    return Math.ceil(text.length / 4);\n  }\n\n  async getModels(): Promise<AIModel[]> {\n    // Return some popular text generation models available on Hugging Face\n    return [\n      {\n        id: 'microsoft/DialoGPT-medium',\n        name: 'DialoGPT Medium',\n        description: 'Conversational AI model by Microsoft',\n        contextLength: 1024,\n        capabilities: ['text', 'chat'],\n        supportsFunctions: false,\n        supportsVision: false,\n      },\n      {\n        id: 'microsoft/DialoGPT-large',\n        name: 'DialoGPT Large',\n        description: 'Large conversational AI model by Microsoft',\n        contextLength: 1024,\n        capabilities: ['text', 'chat'],\n        supportsFunctions: false,\n        supportsVision: false,\n      },\n      {\n        id: 'facebook/blenderbot-400M-distill',\n        name: 'BlenderBot 400M',\n        description: 'Conversational AI model by Meta',\n        contextLength: 512,\n        capabilities: ['text', 'chat'],\n        supportsFunctions: false,\n        supportsVision: false,\n      },\n      {\n        id: 'gpt2',\n        name: 'GPT-2',\n        description: 'OpenAI GPT-2 model',\n        contextLength: 1024,\n        capabilities: ['text', 'completion'],\n        supportsFunctions: false,\n        supportsVision: false,\n      },\n      {\n        id: 'sentence-transformers/all-MiniLM-L6-v2',\n        name: 'All-MiniLM-L6-v2',\n        description: 'Sentence embedding model',\n        contextLength: 512,\n        capabilities: ['embeddings'],\n        supportsFunctions: false,\n        supportsVision: false,\n      },\n    ];\n  }\n\n  async getCapabilities(): Promise<AICapabilities> {\n    return {\n      chat: true,\n      completion: true,\n      embeddings: true,\n      streaming: false, // Limited streaming support\n      functions: false, // Most HF models don't support function calling\n      vision: false, // Limited vision model support\n      fineTuning: true, // Via Hugging Face training API\n      maxContextLength: 2048,\n      supportedOperations: ['chat', 'completion', 'embedding'],\n    };\n  }\n\n  private messagesToPrompt(messages: AIMessage[]): string {\n    // Convert chat messages to a single prompt format\n    return `${messages\n      .map((message) => {\n        switch (message.role) {\n          case 'system':\n            return `System: ${message.content}`;\n          case 'user':\n            return `Human: ${message.content}`;\n          case 'assistant':\n            return `Assistant: ${message.content}`;\n          default:\n            return message.content;\n        }\n      })\n      .join('\\n')}\\nAssistant:`;\n  }\n\n  private async makeRequest(endpoint: string, data: any): Promise<any> {\n    const url = `${this.baseUrl}${endpoint}`;\n\n    const response = await fetch(url, {\n      method: 'POST',\n      headers: {\n        Authorization: `Bearer ${this.options.apiToken}`,\n        'Content-Type': 'application/json',\n        ...this.options.headers,\n      },\n      body: JSON.stringify(data),\n    });\n\n    if (!response.ok) {\n      const errorText = await response.text();\n      throw new Error(`HTTP ${response.status}: ${errorText}`);\n    }\n\n    return response.json();\n  }\n\n  private mapError(error: unknown): AIError {\n    if (error instanceof AIError) {\n      return error;\n    }\n\n    const message = error instanceof Error ? error.message : 'Unknown error';\n\n    // Map common HTTP status codes\n    if (message.includes('401') || message.includes('Unauthorized')) {\n      return new AuthenticationError('huggingface');\n    }\n\n    if (message.includes('429') || message.includes('rate limit')) {\n      return new RateLimitError('huggingface');\n    }\n\n    if (message.includes('404') || message.includes('not found')) {\n      return new ModelNotFoundError(message, 'huggingface');\n    }\n\n    if (message.includes('413') || message.includes('too large')) {\n      return new ContextLengthError('huggingface');\n    }\n\n    return new AIError(message, 'UNKNOWN_ERROR', 'huggingface');\n  }\n}\n"],"names":[],"mappings":";AAwBO,MAAM,oBAA2C;AAAA,EAC9C;AAAA,EACA;AAAA,EAER,YAAY,SAA6B;AACvC,SAAK,UAAU;AAAA,MACb,cAAc;AAAA,MACd,UAAU;AAAA,MACV,cAAc;AAAA,MACd,GAAG;AAAA,IAAA;AAGL,SAAK,UACH,KAAK,QAAQ,YAAY;AAAA,EAC7B;AAAA,EAEA,MAAM,KACJ,UACA,UAAuB,IACF;AACrB,QAAI;AAEF,YAAM,SAAS,KAAK,iBAAiB,QAAQ;AAE7C,YAAM,WAAW,MAAM,KAAK;AAAA,QAC1B,WAAW,QAAQ,SAAS,KAAK,QAAQ,SAAS,KAAK,QAAQ,YAAY;AAAA,QAC3E;AAAA,UACE,QAAQ;AAAA,UACR,YAAY;AAAA,YACV,gBAAgB,QAAQ,aAAa;AAAA,YACrC,aAAa,QAAQ,eAAe;AAAA,YACpC,OAAO,QAAQ,QAAQ;AAAA,YACvB,WACG,QAAQ,eAAe,QAAQ,cAAc,KAAM;AAAA,YACtD,gBAAgB,MAAM,QAAQ,QAAQ,IAAI,IACtC,QAAQ,OACR,QAAQ,OACN,CAAC,QAAQ,IAAI,IACb;AAAA,UAAA;AAAA,UAER,SAAS;AAAA,YACP,WAAW,KAAK,QAAQ;AAAA,YACxB,gBAAgB,KAAK,QAAQ;AAAA,UAAA;AAAA,QAC/B;AAAA,MACF;AAGF,UAAI,MAAM,QAAQ,QAAQ,KAAK,SAAS,CAAC,GAAG,gBAAgB;AAC1D,cAAM,gBAAgB,SAAS,CAAC,EAAE;AAElC,cAAM,UAAU,cAAc,QAAQ,QAAQ,EAAE,EAAE,KAAA;AAElD,eAAO;AAAA,UACL;AAAA,UACA,OACE,QAAQ,SAAS,KAAK,QAAQ,SAAS,KAAK,QAAQ;AAAA,UACtD,cAAc;AAAA,QAAA;AAAA,MAElB;AAEA,YAAM,IAAI;AAAA,QACR;AAAA,QACA;AAAA,QACA;AAAA,MAAA;AAAA,IAEJ,SAAS,OAAO;AACd,YAAM,KAAK,SAAS,KAAK;AAAA,IAC3B;AAAA,EACF;AAAA,EAEA,MAAM,SACJ,QACA,UAA6B,IACR;AACrB,WAAO,KAAK,KAAK,CAAC,EAAE,MAAM,QAAQ,SAAS,OAAA,CAAQ,GAAG;AAAA,MACpD,OAAO,QAAQ;AAAA,MACf,WAAW,QAAQ;AAAA,MACnB,aAAa,QAAQ;AAAA,MACrB,MAAM,QAAQ;AAAA,MACd,GAAG,QAAQ;AAAA,MACX,MAAM,QAAQ;AAAA,MACd,QAAQ,QAAQ;AAAA,MAChB,YAAY,QAAQ;AAAA,IAAA,CACrB;AAAA,EACH;AAAA,EAEA,MAAM,MACJ,MACA,UAA4B,IACA;AAC5B,QAAI;AACF,YAAM,QAAQ,MAAM,QAAQ,IAAI,IAAI,OAAO,CAAC,IAAI;AAChD,YAAM,QAAQ,QAAQ,SAAS;AAE/B,YAAM,WAAW,MAAM,KAAK,YAAY,WAAW,KAAK,IAAI;AAAA,QAC1D,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,WAAW,KAAK,QAAQ;AAAA,UACxB,gBAAgB,KAAK,QAAQ;AAAA,QAAA;AAAA,MAC/B,CACD;AAGD,UAAI;AACJ,UAAI,MAAM,QAAQ,QAAQ,KAAK,MAAM,QAAQ,SAAS,CAAC,CAAC,GAAG;AAEzD,qBAAa,MAAM,QAAQ,IAAI,IAAI,WAAW,CAAC,SAAS,CAAC,CAAC;AAAA,MAC5D,WACE,YACA,OAAO,aAAa,YACpB,SAAS,YACT;AAEA,qBAAa,SAAS;AAAA,MACxB,OAAO;AACL,cAAM,IAAI;AAAA,UACR;AAAA,UACA;AAAA,UACA;AAAA,QAAA;AAAA,MAEJ;AAEA,aAAO;AAAA,QACL;AAAA,QACA;AAAA,MAAA;AAAA,IAEJ,SAAS,OAAO;AACd,YAAM,KAAK,SAAS,KAAK;AAAA,IAC3B;AAAA,EACF;AAAA,EAEA,OAAO,OACL,UACA,UAAuB,IACA;AAGvB,UAAM,WAAW,MAAM,KAAK,KAAK,UAAU,OAAO;AAGlD,UAAM,UAAU,SAAS;AACzB,UAAM,YAAY;AAElB,aAAS,IAAI,GAAG,IAAI,QAAQ,QAAQ,KAAK,WAAW;AAClD,YAAM,QAAQ,QAAQ,MAAM,GAAG,IAAI,SAAS;AAC5C,UAAI,QAAQ,YAAY;AACtB,gBAAQ,WAAW,KAAK;AAAA,MAC1B;AACA,YAAM;AAGN,YAAM,IAAI,QAAQ,CAAC,YAAY,WAAW,SAAS,EAAE,CAAC;AAAA,IACxD;AAAA,EACF;AAAA,EAEA,MAAM,YAAY,MAA+B;AAE/C,WAAO,KAAK,KAAK,KAAK,SAAS,CAAC;AAAA,EAClC;AAAA,EAEA,MAAM,YAAgC;AAEpC,WAAO;AAAA,MACL;AAAA,QACE,IAAI;AAAA,QACJ,MAAM;AAAA,QACN,aAAa;AAAA,QACb,eAAe;AAAA,QACf,cAAc,CAAC,QAAQ,MAAM;AAAA,QAC7B,mBAAmB;AAAA,QACnB,gBAAgB;AAAA,MAAA;AAAA,MAElB;AAAA,QACE,IAAI;AAAA,QACJ,MAAM;AAAA,QACN,aAAa;AAAA,QACb,eAAe;AAAA,QACf,cAAc,CAAC,QAAQ,MAAM;AAAA,QAC7B,mBAAmB;AAAA,QACnB,gBAAgB;AAAA,MAAA;AAAA,MAElB;AAAA,QACE,IAAI;AAAA,QACJ,MAAM;AAAA,QACN,aAAa;AAAA,QACb,eAAe;AAAA,QACf,cAAc,CAAC,QAAQ,MAAM;AAAA,QAC7B,mBAAmB;AAAA,QACnB,gBAAgB;AAAA,MAAA;AAAA,MAElB;AAAA,QACE,IAAI;AAAA,QACJ,MAAM;AAAA,QACN,aAAa;AAAA,QACb,eAAe;AAAA,QACf,cAAc,CAAC,QAAQ,YAAY;AAAA,QACnC,mBAAmB;AAAA,QACnB,gBAAgB;AAAA,MAAA;AAAA,MAElB;AAAA,QACE,IAAI;AAAA,QACJ,MAAM;AAAA,QACN,aAAa;AAAA,QACb,eAAe;AAAA,QACf,cAAc,CAAC,YAAY;AAAA,QAC3B,mBAAmB;AAAA,QACnB,gBAAgB;AAAA,MAAA;AAAA,IAClB;AAAA,EAEJ;AAAA,EAEA,MAAM,kBAA2C;AAC/C,WAAO;AAAA,MACL,MAAM;AAAA,MACN,YAAY;AAAA,MACZ,YAAY;AAAA,MACZ,WAAW;AAAA;AAAA,MACX,WAAW;AAAA;AAAA,MACX,QAAQ;AAAA;AAAA,MACR,YAAY;AAAA;AAAA,MACZ,kBAAkB;AAAA,MAClB,qBAAqB,CAAC,QAAQ,cAAc,WAAW;AAAA,IAAA;AAAA,EAE3D;AAAA,EAEQ,iBAAiB,UAA+B;AAEtD,WAAO,GAAG,SACP,IAAI,CAAC,YAAY;AAChB,cAAQ,QAAQ,MAAA;AAAA,QACd,KAAK;AACH,iBAAO,WAAW,QAAQ,OAAO;AAAA,QACnC,KAAK;AACH,iBAAO,UAAU,QAAQ,OAAO;AAAA,QAClC,KAAK;AACH,iBAAO,cAAc,QAAQ,OAAO;AAAA,QACtC;AACE,iBAAO,QAAQ;AAAA,MAAA;AAAA,IAErB,CAAC,EACA,KAAK,IAAI,CAAC;AAAA;AAAA,EACf;AAAA,EAEA,MAAc,YAAY,UAAkB,MAAyB;AACnE,UAAM,MAAM,GAAG,KAAK,OAAO,GAAG,QAAQ;AAEtC,UAAM,WAAW,MAAM,MAAM,KAAK;AAAA,MAChC,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,eAAe,UAAU,KAAK,QAAQ,QAAQ;AAAA,QAC9C,gBAAgB;AAAA,QAChB,GAAG,KAAK,QAAQ;AAAA,MAAA;AAAA,MAElB,MAAM,KAAK,UAAU,IAAI;AAAA,IAAA,CAC1B;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,YAAY,MAAM,SAAS,KAAA;AACjC,YAAM,IAAI,MAAM,QAAQ,SAAS,MAAM,KAAK,SAAS,EAAE;AAAA,IACzD;AAEA,WAAO,SAAS,KAAA;AAAA,EAClB;AAAA,EAEQ,SAAS,OAAyB;AACxC,QAAI,iBAAiB,SAAS;AAC5B,aAAO;AAAA,IACT;AAEA,UAAM,UAAU,iBAAiB,QAAQ,MAAM,UAAU;AAGzD,QAAI,QAAQ,SAAS,KAAK,KAAK,QAAQ,SAAS,cAAc,GAAG;AAC/D,aAAO,IAAI,oBAAoB,aAAa;AAAA,IAC9C;AAEA,QAAI,QAAQ,SAAS,KAAK,KAAK,QAAQ,SAAS,YAAY,GAAG;AAC7D,aAAO,IAAI,eAAe,aAAa;AAAA,IACzC;AAEA,QAAI,QAAQ,SAAS,KAAK,KAAK,QAAQ,SAAS,WAAW,GAAG;AAC5D,aAAO,IAAI,mBAAmB,SAAS,aAAa;AAAA,IACtD;AAEA,QAAI,QAAQ,SAAS,KAAK,KAAK,QAAQ,SAAS,WAAW,GAAG;AAC5D,aAAO,IAAI,mBAAmB,aAAa;AAAA,IAC7C;AAEA,WAAO,IAAI,QAAQ,SAAS,iBAAiB,aAAa;AAAA,EAC5D;AACF;"}